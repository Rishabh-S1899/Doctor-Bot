{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBb26yzmYNBc"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Arthur-77/QWEN2.5-1.5B-medical-finetuned\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Good Morning Doctor I am facing weakness from a couple of weeks.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a doctor. A patient has visited you, conversate with the patient and gradually reach to conclusion\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "id": "8pJBFKjNYg6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n"
      ],
      "metadata": {
        "id": "neT3JRU7Zurd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_model(destination_folder):\n",
        "    \"\"\"Load the tokenizer and model from the specified checkpoint.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(destination_folder)\n",
        "\n",
        "    # Set pad token to be the same as eos token if not already set\n",
        "\n",
        "    tokenizer.pad_token_id = 151643\n",
        "    tokenizer.eos_token_id = 151645\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        destination_folder,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16  # Resolve dtype warning\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "def generate_response(tokenizer, model, messages):\n",
        "    \"\"\"Generate a response based on the conversation history.\"\"\"\n",
        "    # Prepare input with attention mask\n",
        "    chat_template = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        truncation=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Create attention mask\n",
        "    attention_mask = chat_template.ne(tokenizer.pad_token_id).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=chat_template,\n",
        "        attention_mask=attention_mask,  # Add attention mask\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id  # Explicitly set pad token\n",
        "    )\n",
        "    conversation_message.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "def is_conversation_end(user_message):\n",
        "    \"\"\"Check if the user's message indicates the end of conversation.\"\"\"\n",
        "    end_phrases = [\n",
        "        \"goodbye\",\n",
        "        \"bye\",\n",
        "        \"end conversation\",\n",
        "        \"that's all\",\n",
        "        \"thank you\",\n",
        "        \"thanks\"\n",
        "    ]\n",
        "    return any(phrase in user_message.lower() for phrase in end_phrases)\n",
        "\n",
        "def main():\n",
        "    # Load the model\n",
        "    destination_folder = \"Arthur-77/QWEN2.5-1.5B-medical-finetuned\"\n",
        "    tokenizer, model = load_model(destination_folder)\n",
        "\n",
        "    # Initial system message\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an experienced doctor. Diagnose the problem faced by the patient based on the symptoms provided by them. Ask for any additional inputs if required to diagnose the problem. If you are not sure say seek medical attention.\",\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Conversational loop\n",
        "    print(\"Doctor Bot: Hello! How are you feeling today?\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Add user message to conversation history\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Check for conversation end\n",
        "        if is_conversation_end(user_input):\n",
        "            print(\"Doctor Bot: Goodbye! Take care of yourself.\")\n",
        "            break\n",
        "\n",
        "        # Generate and print bot response\n",
        "        bot_response = generate_response(tokenizer, model, messages)\n",
        "        print(\"Doctor Bot:\", bot_response)\n",
        "\n",
        "        # Add bot response to conversation history\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xD_H3HPOaONa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}